{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.optim import Adam \n",
    "\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to encode the number and names in the token and then create one hot encoding where the target word is 1 and rest everything is 0 \n",
    "\n",
    "# we will create embeddings for Troll2 is great Gymkata and each time we have a word we put it 1 \n",
    "\n",
    "input = torch.tensor([[1.,0.,0.,0.],\n",
    "                     [0.,1.,0.,0.],\n",
    "                     [0.,0.,1.,0.],\n",
    "                     [0.,0.,0.,1.]])\n",
    "\n",
    "\n",
    "labels = torch.tensor([[0.,1.,0.,0.],\n",
    "                     [0.,0.,1.,0.],\n",
    "                     [0.,0.,0.,1.],\n",
    "                     [0.,1.,0.,0.]])\n",
    "\n",
    "\n",
    "Dataset = TensorDataset(input,labels)\n",
    "\n",
    "meradata = DataLoader(Dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Wordembedding(L.LightningModule):\n",
    "    # we need to select distributions from which we can select values in \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # we need to set a range of min and max distribution \n",
    "        \n",
    "        min_value = -0.5\n",
    "        max_value = 0.5\n",
    "        \n",
    "        self.input1_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.input1_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.input2_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.input2_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.input3_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.input3_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.input4_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.input4_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "\n",
    "        self.output1_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.output1_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.output2_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.output2_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.output3_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.output3_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.output4_w1 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        self.output4_w2 =  nn.Parameter(Uniform(min_value,max_value).sample()) # this will uniformly select values from -0.5 to 0.5 for the values \n",
    "        \n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # now working on forward function \n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        input = input[0]\n",
    "        \n",
    "        input_to_top_hidden = ((input[0]*self.input1_w1)+\n",
    "                               (input[1]*self.input2_w1)+\n",
    "                               (input[2]*self.input3_w1)+\n",
    "                               (input[3*self.input4_w1]))\n",
    "        \n",
    "        input_bottom_to_hidden =((input[0]*self.input1_w2)+\n",
    "                               (input[1]*self.input2_w2)+\n",
    "                               (input[2]*self.input3_w2)+\n",
    "                               (input[3*self.input4_w2]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        output1 = ((input_to_top_hidden*self.output1_w1)+\n",
    "                   (input_bottom_to_hidden*self.output1_w2))\n",
    "        \n",
    "        output2 = ((input_to_top_hidden*self.output2_w1)+\n",
    "                   (input_bottom_to_hidden*self.output2_w2))\n",
    "        \n",
    "        output3 = ((input_to_top_hidden*self.output3_w1)+\n",
    "                   (input_bottom_to_hidden*self.output3_w2))\n",
    "\n",
    "        output4 = ((input_to_top_hidden*self.output4_w1)+\n",
    "                   (input_bottom_to_hidden*self.output4_w2))\n",
    "        \n",
    "        \n",
    "# we have completed the softmax function in the semi final layer of the  NN \n",
    "\n",
    "        output_pre_softmax = torch.stack([output1,output2,output3,output4])\n",
    "        return(output_pre_softmax)\n",
    "    \n",
    "    # optimiser functions \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(),lr = 0.1)\n",
    "    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        input_i , label_i = batch\n",
    "        output_i = self.forward(input_i)\n",
    "        loss = self.loss(output_i,label_i[0])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meramodel = Wordembedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before Optimisation The value of all the weights and parameters is : \n",
      " \n",
      "input1_w1 tensor(-0.3599)\n",
      "input1_w2 tensor(-0.4138)\n",
      "input2_w1 tensor(-0.1273)\n",
      "input2_w2 tensor(-0.3227)\n",
      "input3_w1 tensor(-0.0906)\n",
      "input3_w2 tensor(0.4151)\n",
      "input4_w1 tensor(0.0509)\n",
      "input4_w2 tensor(-0.4484)\n",
      "output1_w1 tensor(0.1574)\n",
      "output1_w2 tensor(-0.4393)\n",
      "output2_w1 tensor(-0.0390)\n",
      "output2_w2 tensor(0.4564)\n",
      "output3_w1 tensor(-0.2010)\n",
      "output3_w2 tensor(-0.0263)\n",
      "output4_w1 tensor(-0.1235)\n",
      "output4_w2 tensor(0.4545)\n"
     ]
    }
   ],
   "source": [
    "print(\"before Optimisation The value of all the weights and parameters is : \\n \")\n",
    "\n",
    "for name, param in Meramodel.named_parameters():\n",
    "    print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to organise this data into dataframe for usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>token</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.359934</td>\n",
       "      <td>-0.413833</td>\n",
       "      <td>Parth</td>\n",
       "      <td>input1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127308</td>\n",
       "      <td>-0.322701</td>\n",
       "      <td>is</td>\n",
       "      <td>input2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.090627</td>\n",
       "      <td>0.415122</td>\n",
       "      <td>Great</td>\n",
       "      <td>input2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050879</td>\n",
       "      <td>-0.448396</td>\n",
       "      <td>Dear</td>\n",
       "      <td>input4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w1        w2  token   input\n",
       "0 -0.359934 -0.413833  Parth  input1\n",
       "1 -0.127308 -0.322701    is   input2\n",
       "2 -0.090627  0.415122  Great  input2\n",
       "3  0.050879 -0.448396   Dear  input4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \n",
    "    \"w1\": [Meramodel.input1_w1.item(),\n",
    "          Meramodel.input2_w1.item(),\n",
    "          Meramodel.input3_w1.item(),\n",
    "          Meramodel.input4_w1.item()],\n",
    "    \n",
    "    \n",
    "    \"w2\": [Meramodel.input1_w2.item(),\n",
    "          Meramodel.input2_w2.item(),\n",
    "          Meramodel.input3_w2.item(),\n",
    "          Meramodel.input4_w2.item()],    \n",
    "    \n",
    "    \n",
    "    \"token\" : [\"Parth\", \"is \", \"Great\" , \"Dear\"],\n",
    "    \"input\" : [ \"input1\", \"input2\", \"input2\" , \"input4\"]\n",
    "    }\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.05087888240814209, -0.4483962655067444, 'Dear')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.scatterplot(data = df , x = 'w1', y = 'w2')\n",
    "\n",
    "plt.text(df.w1[0], df.w2[0], df.token[0],\n",
    "         horizontalalignment = 'left',\n",
    "         size = 'medium',\n",
    "         color = 'black',\n",
    "         weight = 'semibold')\n",
    "\n",
    "\n",
    "\n",
    "plt.text(df.w1[1], df.w2[1], df.token[1],\n",
    "         horizontalalignment = 'left',\n",
    "         size = 'medium',\n",
    "         color = 'black',\n",
    "         weight = 'semibold')\n",
    "\n",
    "\n",
    "plt.text(df.w1[2], df.w2[2], df.token[2],\n",
    "         horizontalalignment = 'left',\n",
    "         size = 'medium',\n",
    "         color = 'black',\n",
    "         weight = 'semibold')\n",
    "\n",
    "\n",
    "\n",
    "plt.text(df.w1[3], df.w2[3], df.token[3],\n",
    "         horizontalalignment = 'left',\n",
    "         size = 'medium',\n",
    "         color = 'black',\n",
    "         weight = 'semibold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | loss         | CrossEntropyLoss | 0      | train\n",
      "  | other params | n/a              | 16     | n/a  \n",
      "----------------------------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/home/parthshr370/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/parthshr370/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMeramodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeradata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1030\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    162\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/core/module.py:1308\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1279\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1283\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/torch/optim/adam.py:148\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 148\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    151\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/parthpython/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 80\u001b[0m, in \u001b[0;36mWordembedding.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m,batch,batch_idx):\n\u001b[1;32m     79\u001b[0m     input_i , label_i \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 80\u001b[0m     output_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(output_i,label_i[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[48], line 44\u001b[0m, in \u001b[0;36mWordembedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     41\u001b[0m     input_to_top_hidden \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput1_w1)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     42\u001b[0m                            (\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput2_w1)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     43\u001b[0m                            (\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput3_w1)\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m---> 44\u001b[0m                            (\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput4_w1\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     46\u001b[0m     input_bottom_to_hidden \u001b[38;5;241m=\u001b[39m((\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput1_w2)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     47\u001b[0m                            (\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput2_w2)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     48\u001b[0m                            (\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput3_w2)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     49\u001b[0m                            (\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput4_w2]))\n\u001b[1;32m     54\u001b[0m     output1 \u001b[38;5;241m=\u001b[39m ((input_to_top_hidden\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput1_w1)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     55\u001b[0m                (input_bottom_to_hidden\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput1_w2))\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "\n",
    "trainer.fit(Meramodel,train_dataloaders=meradata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parthpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
